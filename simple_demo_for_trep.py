# -*- coding: utf-8 -*-
"""Simple_DEMO_for_TrEP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1BaL39sEruawcoMswVWjCOg4vFYAnt9i2

#Environment
"""

import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torchvision.datasets.mnist import MNIST
import torchvision.transforms as transforms
from torch.utils.data import DataLoader
from torch.autograd import Variable
import copy
import numpy as np
import argparse
from matplotlib import pyplot as plt
from PIL import Image
import math
from typing import Tuple

import torch
from torch import nn, Tensor
import torch.nn.functional as F
from torch.nn import TransformerEncoder, TransformerEncoderLayer,TransformerDecoder,TransformerDecoderLayer
from torch.utils.data import dataset


import pickle
import pandas as pd
from logging import raiseExceptions
# pytorch mlp for binary classification
import torch
import numpy as np
from numpy import vstack
from pandas import read_csv
# from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score,f1_score,roc_auc_score,matthews_corrcoef,auc,precision_score
from torch.utils.data import Dataset
from torch.utils.data import DataLoader
from torch.utils.data import random_split
from torch import Tensor
from torch.nn import Linear
from torch.nn import ReLU
from torch.nn import Sigmoid,Softmax
from torch.nn import Module
from torch.optim import SGD,Adam
from torch.nn import BCELoss
from torch.nn.init import kaiming_uniform_
from torch.nn.init import xavier_uniform_
from torch import nn
from sklearn.preprocessing import OneHotEncoder
import time



"""#Main

##Data Engineering
"""

def data_to_tabular_feature_1_iccv(data):
    time_l =data['box'].shape[1]
    lent = len(data['crossing'])
    data_array = np.concatenate(
        [data['center'].reshape(lent, -1), data['box'].reshape(lent, -1),
         data['speed'].reshape(lent, -1)], axis=1)
    col = ['center_1_'+str(i) for i in range(time_l)]+['center_2_'+str(i) for i in range(time_l)]+['box_org_1_'+str(i) for i in range(time_l)]+['box_org_2_'+str(i) for i in range(time_l)]+['box_org_3_'+str(i) for i in range(time_l)]+['box_org_4_'+str(i) for i in range(time_l)]+['speed_'+str(i) for i in range(time_l)]

    df = pd.DataFrame(data_array,columns=col)
    df['crossing'] = data['crossing']
    return df

def data_to_tabular_iccv(data):
    len_ts =data['box'].shape[1]
    lent = len(data['crossing'])
    data_array = np.concatenate(
        [data['center'].reshape(lent, -1), data['box'].reshape(lent, -1), data['box_org'].reshape(lent, -1),
         data['speed'].reshape(lent, -1)], axis=1)
    col = ['center_1_'+str(i) for i in range(len_ts)]+['center_2_'+str(i) for i in range(len_ts)]+['box_1_'+str(i) for i in range(len_ts)]+['box_2_'+str(i) for i in range(len_ts)]+['box_3_'+str(i) for i in range(len_ts)]+['box_4_'+str(i) for i in range(len_ts)]+['box_org_1_'+str(i) for i in range(len_ts)]+['box_org_2_'+str(i) for i in range(len_ts)]+['box_org_3_'+str(i) for i in range(len_ts)]+['box_org_4_'+str(i) for i in range(len_ts)]+['speed_'+str(i) for i in range(len_ts)]

    df = pd.DataFrame(data_array,columns=col)
    df['crossing'] = data['crossing']
    return df

def data_to_tabular_iccv_transformer(data):
    len_ts =data['box'].shape[1]
    lent = len(data['crossing'])
    data_array = np.concatenate(
        [data['center'], data['box'], data['box_org'],
         data['speed']], axis=2)
    #col = ['center_1_'+str(i) for i in range(len_ts)]+['center_2_'+str(i) for i in range(len_ts)]+['box_1_'+str(i) for i in range(len_ts)]+['box_2_'+str(i) for i in range(len_ts)]+['box_3_'+str(i) for i in range(len_ts)]+['box_4_'+str(i) for i in range(len_ts)]+['box_org_1_'+str(i) for i in range(len_ts)]+['box_org_2_'+str(i) for i in range(len_ts)]+['box_org_3_'+str(i) for i in range(len_ts)]+['box_org_4_'+str(i) for i in range(len_ts)]+['speed_'+str(i) for i in range(len_ts)]

    #df = pd.DataFrame(data_array,columns=col)
    #df['crossing'] = data['crossing']
    return data_array,data['crossing']

def data_to_tabular_iccv_transformer_feature_1(data):
    len_ts =data['box'].shape[1]
    lent = len(data['crossing'])
    data_array = np.concatenate(
        [data['center'], data['box'], 
         data['speed']], axis=2)
    #col = ['center_1_'+str(i) for i in range(len_ts)]+['center_2_'+str(i) for i in range(len_ts)]+['box_1_'+str(i) for i in range(len_ts)]+['box_2_'+str(i) for i in range(len_ts)]+['box_3_'+str(i) for i in range(len_ts)]+['box_4_'+str(i) for i in range(len_ts)]+['box_org_1_'+str(i) for i in range(len_ts)]+['box_org_2_'+str(i) for i in range(len_ts)]+['box_org_3_'+str(i) for i in range(len_ts)]+['box_org_4_'+str(i) for i in range(len_ts)]+['speed_'+str(i) for i in range(len_ts)]

    #df = pd.DataFrame(data_array,columns=col)
    #df['crossing'] = data['crossing']
    return data_array,data['crossing']

"""##Dataset"""

class tabular_transformer(Dataset):
    # load the dataset
    def __init__(self, X,y):
        # load the csv file as a dataframe
        # df = read_csv(path)

        # store the inputs and outputs
        self.y = y
        
        self.X = X

        # ensure input data is floats
        self.X = self.X.astype('float32')
        # label encode target and ensure the values are floats
        #self.y = OneHotEncoder(sparse=False).fit_transform(self.y.reshape(-1,1))
        self.y = self.y.astype('int')
        # self.y_1 = np.subtract(np.ones(self.y.shape),self.y)
        # self.y = np.concatenate((self.y,self.y_1),axis=1)
        #self.y = self.y.reshape((len(self.y), 1))
        self.X = torch.tensor(self.X).to('cuda')
        self.y = torch.tensor(self.y).reshape(len(self.y)).to('cuda')
        #self.y = torch.argmax(self.y,1)
    # number of rows in the dataset
    def __len__(self):
        return len(self.X)

    # get a row at an index
    def __getitem__(self, idx):
        return [self.X[idx], self.y[idx]]

    # get indexes for train and test rows
    def get_splits(self, n_test=0.33):
        # determine sizes
        test_size = round(n_test * len(self.X))
        train_size = len(self.X) - test_size
        # calculate the split
        return random_split(self, [train_size, test_size])

def prepare_data_transformer(train,test,bs = 64):
    # load the dataset
    X_train,y_train = data_to_tabular_iccv_transformer_feature_1(train)
    X_test,y_test = data_to_tabular_iccv_transformer_feature_1(test)
    # calculate split
    train_data = tabular_transformer(X_train,y_train)
    test_data = tabular_transformer(X_test,y_test)
    # prepare data loaders
    train_dl = DataLoader(train_data, batch_size=bs, shuffle=True)
    test_dl = DataLoader(test_data, batch_size=bs, shuffle=False)
    return train_dl, test_dl

def prepare_data_transformer_jaad(train,test,bs = 64):
    # load the dataset
    X_train,y_train = data_to_tabular_iccv_transformer(train)
    X_test,y_test = data_to_tabular_iccv_transformer(test)
    # calculate split
    train_data = tabular_transformer(X_train,y_train)
    test_data = tabular_transformer(X_test,y_test)
    # prepare data loaders
    train_dl = DataLoader(train_data, batch_size=bs, shuffle=True)
    test_dl = DataLoader(test_data, batch_size=bs, shuffle=False)
    return train_dl, test_dl

"""##base model"""

class PositionalEncoding(nn.Module):

    def __init__(self, d_model: int, dropout: float = 0.1, max_len: int = 5000):
        super().__init__()
        self.dropout = nn.Dropout(p=dropout)

        position = torch.arange(max_len).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))
        pe = torch.zeros(max_len, 1, d_model)
        pe[:, 0, 0::2] = torch.sin(position * div_term)
        pe[:, 0, 1::2] = torch.cos(position * div_term)
        self.register_buffer('pe', pe)

    def forward(self, x: Tensor) -> Tensor:
        """
        Args:
            x: Tensor, shape [seq_len, batch_size, embedding_dim]
        """
        x = x + self.pe[:x.size(0)]
        return self.dropout(x)



class TransformerModel_no_softmax(nn.Module):

    def __init__(self, ntoken: int, d_model: int, nhead: int, d_hid: int,
                 nlayers: int, dropout: float = 0.5):
        super().__init__()
        self.model_type = 'Transformer'
        self.pos_encoder = PositionalEncoding(d_model, dropout)
        encoder_layers = TransformerEncoderLayer(d_model, nhead, d_hid, dropout)
        self.transformer_encoder = TransformerEncoder(encoder_layers, nlayers)
        self.encoder = nn.Linear(7, d_model)
        self.d_model = d_model
        self.decoder = nn.Linear(d_model*14, ntoken)
        #self.op_act = Softmax(dim=1)
        #self.init_weights()
        self.readout = nn.Flatten()
    # def init_weights(self) -> None:
    #     initrange = 0.1
    #     #self.encoder.weight.data.uniform_(-initrange, initrange)
    #     self.decoder.bias.data.zero_()
    #     self.decoder.weight.data.uniform_(-initrange, initrange)
    def init_params(self):
        for layer in self.children():
            if hasattr(layer, 'reset_parameters'):
                layer.reset_parameters()
            else:
                for ll in layer.children():
                    if hasattr(layer, 'reset_parameters'):
                        layer.reset_parameters()

    def forward(self, src: Tensor) -> Tensor:
        """
        Args:
            src: Tensor, shape [seq_len, batch_size]
            src_mask: Tensor, shape [seq_len, seq_len]

        Returns:
            output Tensor of shape [seq_len, batch_size, ntoken]
        """
        src = self.encoder(src) #* math.sqrt(self.d_model)
        src = self.pos_encoder(src)
        output = self.transformer_encoder(src)
        output = self.readout(output)
        output = self.decoder(output)
        #output = self.op_act(output)
        return output





"""##EVIDENTIAL_LOSS"""

import torch
import torch.nn.functional as F
def get_device():
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda:0" if use_cuda else "cpu")
    return device


def one_hot_embedding(labels, num_classes=10):
    # Convert to One Hot Encoding
    y = torch.eye(num_classes)
    return y[labels]


def relu_evidence(y):
    return F.relu(y)


def exp_evidence(y):
    return torch.exp(torch.clamp(y, -10, 10))


def softplus_evidence(y):
    return F.softplus(y)


def kl_divergence(alpha, num_classes, device=None):
    if not device:
        device = get_device()
    ones = torch.ones([1, num_classes], dtype=torch.float32, device=device)
    sum_alpha = torch.sum(alpha, dim=1, keepdim=True)
    first_term = (
        torch.lgamma(sum_alpha)
        - torch.lgamma(alpha).sum(dim=1, keepdim=True)
        + torch.lgamma(ones).sum(dim=1, keepdim=True)
        - torch.lgamma(ones.sum(dim=1, keepdim=True))
    )
    second_term = (
        (alpha - ones)
        .mul(torch.digamma(alpha) - torch.digamma(sum_alpha))
        .sum(dim=1, keepdim=True)
    )
    kl = first_term + second_term
    return kl


def loglikelihood_loss(y, alpha, device=None):
    if not device:
        device = get_device()
    y = y.to(device)
    alpha = alpha.to(device)
    S = torch.sum(alpha, dim=1, keepdim=True)
    loglikelihood_err = torch.sum((y - (alpha / S)) ** 2, dim=1, keepdim=True)
    loglikelihood_var = torch.sum(
        alpha * (S - alpha) / (S * S * (S + 1)), dim=1, keepdim=True
    )
    loglikelihood = loglikelihood_err + loglikelihood_var
    return loglikelihood


def mse_loss(y, alpha, epoch_num, num_classes, annealing_step, device=None):
    if not device:
        device = get_device()
    y = y.to(device)
    alpha = alpha.to(device)
    loglikelihood = loglikelihood_loss(y, alpha, device=device)

    annealing_coef = torch.min(
        torch.tensor(1.0, dtype=torch.float32),
        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),
    )

    kl_alpha = (alpha - 1) * (1 - y) + 1
    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)
    return loglikelihood + kl_div


def edl_loss(func, y, alpha, epoch_num, num_classes, annealing_step, device=None):
    y = y.to(device)
    alpha = alpha.to(device)
    S = torch.sum(alpha, dim=1, keepdim=True)

    A = torch.sum(y * (func(S) - func(alpha)), dim=1, keepdim=True)

    annealing_coef = torch.min(
        torch.tensor(1.0, dtype=torch.float32),
        torch.tensor(epoch_num / annealing_step, dtype=torch.float32),
    )

    kl_alpha = (alpha - 1) * (1 - y) + 1
    kl_div = annealing_coef * kl_divergence(kl_alpha, num_classes, device=device)
    return A + kl_div


def edl_mse_loss(output, target, epoch_num, num_classes, annealing_step, device=None):
    if not device:
        device = get_device()
    evidence = relu_evidence(output)
    alpha = evidence + 1
    loss = torch.mean(
        mse_loss(target, alpha, epoch_num, num_classes, annealing_step, device=device)
    )
    return loss


def edl_log_loss(output, target, epoch_num, num_classes, annealing_step, device=None):
    if not device:
        device = get_device()
    evidence = relu_evidence(output)
    alpha = evidence + 1
    loss = torch.mean(
        edl_loss(
            torch.log, target, alpha, epoch_num, num_classes, annealing_step, device
        )
    )
    return loss


def edl_digamma_loss(
    output, target, epoch_num, num_classes, annealing_step, device=None
):
    if not device:
        device = get_device()
    evidence = relu_evidence(output)
    alpha = evidence + 1
    loss = torch.mean(
        edl_loss(
            torch.digamma, target, alpha, epoch_num, num_classes, annealing_step, device
        )
    )
    return loss

"""##train_model"""

# from helpers import get_device, one_hot_embedding
# from losses import relu_evidence


def train_model(
    model,
    dataloaders,
    num_classes,
    criterion,
    optimizer,
    scheduler=None,
    num_epochs=25,
    device=None,
    uncertainty=False,
):

    since = time.time()
    model.init_params()
    if not device:
        device = get_device()

    best_model_wts = copy.deepcopy(model.state_dict())
    best_acc = 0.0

    losses = {"loss": [], "phase": [], "epoch": []}
    accuracy = {"accuracy": [], "phase": [], "epoch": []}
    evidences = {"evidence": [], "type": [], "epoch": []}

    for epoch in range(num_epochs):
        print("Epoch {}/{}".format(epoch, num_epochs - 1))
        print("-" * 10)

        # Each epoch has a training and validation phase
        for phase in ["train", "val"]:
            if phase == "train":
                print("Training...")
                model.train()  # Set model to training mode
            else:
                print("Validating...")
                model.eval()  # Set model to evaluate mode

            running_loss = 0.0
            running_corrects = 0.0
            correct = 0
            running_preds = torch.empty(0)
            running_labels = torch.empty(0)

            # Iterate over data.
            for i, (inputs, labels) in enumerate(dataloaders[phase]):

                inputs = inputs.to(device)
                labels = labels.to(device)

                # zero the parameter gradients
                optimizer.zero_grad()

                # forward
                # track history if only in train
                with torch.set_grad_enabled(phase == "train"):

                    if uncertainty:
                        y = one_hot_embedding(labels, num_classes)
                        y = y.to(device)
                        outputs = model(inputs)
                        #_, preds = torch.max(outputs, 1)
                        #preds = outputs[:,1]

                        loss = criterion(
                            outputs, y.float(), epoch, num_classes, 10, device
                        )

                        # match = torch.reshape(torch.eq(preds, labels).float(), (-1, 1))
                        # acc = torch.mean(match)
                        evidence = relu_evidence(outputs)
                        alpha = evidence + 1
                        #u = num_classes / torch.sum(alpha, dim=1, keepdim=True)
                        prob = alpha / torch.sum(alpha, dim=1, keepdim=True)
                        preds = prob[:,1]
                        # total_evidence = torch.sum(evidence, 1, keepdim=True)
                        # mean_evidence = torch.mean(total_evidence)
                        # mean_evidence_succ = torch.sum(
                        #     torch.sum(evidence, 1, keepdim=True) * match
                        # ) / torch.sum(match + 1e-20)
                        # mean_evidence_fail = torch.sum(
                        #     torch.sum(evidence, 1, keepdim=True) * (1 - match)
                        # ) / (torch.sum(torch.abs(1 - match)) + 1e-20)

                    else:
                        outputs = model(inputs)
                        #_, preds = torch.max(outputs, 1)
                        preds = outputs[:,1]
                        loss = criterion(outputs, labels)

                    if phase == "train":
                        loss.backward()
                        optimizer.step()

                # statistics
                running_loss += loss.item() * inputs.size(0)
                #print(preds)
                running_corrects += torch.sum(torch.round(preds.detach()) == labels.data)
                running_preds = torch.concat((running_preds,preds.to('cpu')),0)
                running_labels = torch.concat((running_labels,labels.to('cpu')),0)
            if scheduler is not None:
                if phase == "train":
                    scheduler.step()

            epoch_loss = running_loss / len(dataloaders[phase].dataset)
            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)

            losses["loss"].append(epoch_loss)
            losses["phase"].append(phase)
            losses["epoch"].append(epoch)
            accuracy["accuracy"].append(epoch_acc.item())
            accuracy["epoch"].append(epoch)
            accuracy["phase"].append(phase)
            actuals = running_labels.detach().numpy()

            predictions  = running_preds.detach().numpy()
            # print(predictions)
            # print(actuals)
            acc = accuracy_score(actuals, np.round(predictions))
            f1 = f1_score(actuals,np.round(predictions))
            roc = roc_auc_score(actuals,predictions)
            prec = precision_score(actuals,np.round(predictions))
            print(
                "{} loss: {:.4f} acc: {:.4f} acc2:{:.4f} f1:{:.4f} roc:{:.4f} precision:{:.4f}".format(
                    phase.capitalize(), epoch_loss, epoch_acc,acc,f1,roc,prec
                )
            )

            # deep copy the model
            if phase == "val" and epoch_acc > best_acc:
                best_acc = epoch_acc
                best_f1 =f1
                best_roc = roc
                best_prec = prec
                best_model_wts = copy.deepcopy(model.state_dict())

        print()

    time_elapsed = time.time() - since
    print(
        "Training complete in {:.0f}m {:.0f}s".format(
            time_elapsed // 60, time_elapsed % 60
        )
    )
    print("Best val Acc: {:4f} F1: {:4f} AUC:{:4f} Percision:{:4f} ".format(best_acc,best_f1,best_roc,best_prec))

    # load best model weights
    model.load_state_dict(best_model_wts)
    metrics = (losses, accuracy)

    return model, metrics



"""#PIE

##Read preproccesed data
"""

pie_train_track = '/content/drive/MyDrive/TASI/2023_AAAI/data_all_you_need/pie_train_track.pkl'
with open(pie_train_track,'rb') as f:
  data_train = pickle.load(f)
pie_val_track = '/content/drive/MyDrive/TASI/2023_AAAI/data_all_you_need/pie_val_track.pkl'
with open(pie_val_track,'rb') as f:
  data_val = pickle.load(f)

"""##Trainning"""

train_dl, test_dl = prepare_data_transformer(data_train,data_val,32)
#from train import train_model

num_epochs = 1000
use_uncertainty = True
num_classes = 2
criterion = edl_digamma_loss
model = TransformerModel_no_softmax(2,8, 2, 16, 1, 0.1).to('cuda')
dataloaders = {'train':train_dl,'val':test_dl}
optimizer = optim.Adam(model.parameters(), lr=0.005, weight_decay=1e-6)
#exp_lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)
exp_lr_scheduler =None

device = get_device()
model, metrics = train_model(
    model,
    dataloaders,
    num_classes,
    criterion,
    optimizer,
    scheduler=exp_lr_scheduler,
    num_epochs=num_epochs,
    device=device,
    uncertainty=use_uncertainty,
)
# torch.save(model.state_dict(), '/content/drive/MyDrive/TASI/2023_AAAI/weights/evidential_pie.pt')



"""##Evaluate with Pre-trained model"""

# import torch
pie_test_track = '/content/drive/MyDrive/TASI/2023_AAAI/data_all_you_need/pie_test_track.pkl'
with open(pie_test_track,'rb') as f:
  data_test = pickle.load(f)
model = torch.load('/content/drive/MyDrive/TASI/2023_AAAI/weights/evidential_pie.pt')
train_dl,test_dl = prepare_data_transformer(data_train,data_test)

def flatten_list(_2d_list):
    flat_list = []
    # Iterate through the outer list
    for element in _2d_list:
        if (type(element) is list) or (type(element) is np.ndarray):
            # If the element is of type list, iterate through the sublist
            for item in element:
                flat_list.append(item)
        else:
            flat_list.append(element)
    return np.asarray(flat_list)

pred_all,prob_all,label_all,uncertainty_all = [],[],[],[]
device = 'cuda'
num_classes=2
for i, (inputs, labels) in enumerate(test_dl):

    inputs = inputs.to(device)
    labels = labels.to(device)
    output = model(inputs)
    evidence = relu_evidence(output)
    alpha = evidence + 1
    uncertainty = num_classes / torch.sum(alpha, dim=1, keepdim=True)
    _, preds = torch.max(output, 1)
    prob = alpha / torch.sum(alpha, dim=1, keepdim=True)
    output = output.flatten()
    prob = prob[:,1]
    preds = preds.flatten()

    pred_all.append(preds.cpu().numpy())
    prob_all.append(prob.cpu().detach().numpy())
    label_all.append(labels.cpu().numpy())
    uncertainty_all.append(uncertainty.cpu().detach().numpy())
pred_all = flatten_list(pred_all)
prob_all = flatten_list(prob_all)
label_all = flatten_list(label_all)
uncertainty_all = flatten_list(uncertainty_all)

accs,f1s,aucs,precs =[],[],[],[]
pers = []
threshs = []
for i in range(11):
  thresh = i*.1
  uncertainty_all = np.asarray(uncertainty_all)
  #ind = [i for i in range(len(uncertainty_all))]
  ind  =  np.where((uncertainty_all<=thresh) & (uncertainty_all>thresh-.1))[0]#&(uncertainty_all>thresh-.1))
  #ind  =  np.where((uncertainty_all<=thresh) )[0]#&(uncertainty_all>thresh-.1))

  if len(ind) >3:
    threshs.append(thresh)
    true = label_all[ind]
    op = prob_all[ind]
    pers.append(len(ind)/len(uncertainty_all))
    accs.append(accuracy_score(true,np.round(op)))
    f1s.append(f1_score(true,np.round(op)))
    aucs.append(roc_auc_score(true,op))
    precs.append(precision_score(true,np.round(op)))

ax1 = plt.subplot()
l1= ax1.plot(threshs,precs,marker = '.',label = 'Percision')
ax1.plot(threshs,f1s,marker = '.',label = 'F1')
ax1.plot(threshs,accs,marker = '.',label = 'Accuracy')
ax1.plot(threshs,aucs,marker = '.',label = 'AUC')
ax1.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25),ncol=4, fancybox=True, shadow=True)
ax1.set_ylim(0,1.05)
ax1.set_ylabel('Metrics',color = 'red',fontsize=20)
ax2 = ax1.twinx()
l2= ax2.bar(threshs,pers,width =.09,color = 'cornflowerblue',align='center',edgecolor='black',alpha=.8)
ax2.set_ylim(0,1.05)
ax2.set_ylabel('Percentage',color = 'cornflowerblue',fontsize =20)
ax1.set_xlabel('Uncertainty',fontsize=20)
# plt.legend([l1, l2], ["speed", "acceleration"])
ax2.set_xticks([.1*i for i in range(11)])
plt.title('Metrics vs. Uncertainty')
# plt.show()
# plt.savefig('Metrics vs. Uncertainty jaad final.png',dpi = 400,)